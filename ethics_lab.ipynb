{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "prev_pub_hash": "9e54548042cc959abb34bd633a9f51784fd2555c0dc1ae36f4d6f1c95b8c4f14"
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "<p style=\"text-align:center\">\n    <a href=\"https://skills.network\" target=\"_blank\">\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n    </a>\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Hands-on Lab: Considerations for Data Professionals using GenAI\n\n**Estimated time needed:** 45 minutes \n\n## Overview  \n\nIn this lab, you will assess and reinforce your understanding of key principles related to the ethical deployment of generative AI, specifically focusing on transparency, fairness, responsibility, accountability, and reliability.  \n\nYou will be presented with scenarios, and you are expected to provide a solution for the question based on the scenario. To help you with the solutions, a hint is provided for each exercise. \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "---\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Learning Objectives \n\nAfter completing this lab, you will be able to: \n\n - Maintain transparency and fairness in your AI system \n\n - Ensure accountability in the deployment of AI chatbot \n\n - Enhance the reliability of your AI model to ensure accurate product descriptions \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "---\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 1:  \n\nYou are developing a generative AI system that creates personalized content recommendations for users. The system seems to consistently recommend content that aligns with certain cultural and demographic biases.  \n\nUsers from diverse backgrounds are expressing concern about the lack of transparency and fairness in the recommendations.  \n\nHow do you maintain transparency and fairness in your AI system? \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<i>Type your response here</i>\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "To maintain transparency and fairness in a generative AI system that provides personalized content recommendations, you can adopt the following strategies:\n1. Conduct Bias Audits and Fairness Testing\nRegularly audit your system for biases by analyzing recommendations across different demographic groups (e.g., age, gender, ethnicity, cultural background). Identify any patterns of bias in the content being recommended and ensure diverse representation.\nPerform fairness testing using fairness metrics like disparate impact analysis, which compares outcomes across different groups.\n2. Diversify Training Data\nEnsure that the training data used for model development represents a broad range of cultures, languages, demographics, and preferences. This reduces the risk of the model learning biased or skewed patterns from limited or homogeneous data sources.\n3. Incorporate Explainability Mechanisms\nImplement explainable AI (XAI) techniques that help users understand why specific recommendations are being made. For example, provide users with insights into which data points or preferences led to the recommendation, and allow them to adjust these factors if desired.\nMake the model’s decision-making process transparent by showing key influencing factors in a user-friendly way.\n4. Enable User Control and Feedback\nAllow users to personalize recommendations by explicitly stating their preferences or giving feedback on whether the recommendation was useful. Incorporating feedback loops into the system enables users to have more control over their experience and ensures their voices are heard.\nImplement a system that allows users to flag content they find problematic or biased, triggering a review of the model’s decisions.\n5. Apply Fairness Constraints\nIntroduce fairness constraints in the recommendation algorithms that ensure equitable representation of different groups. For instance, impose diversity rules to ensure that recommended content covers a range of perspectives.\nUse debiasing techniques such as reweighting, adversarial debiasing, or fairness-aware recommendation algorithms to actively reduce bias in model outputs.\n6. Communicate Ethical Practices\nBe transparent about the data used for training, the algorithms in place, and how fairness is prioritized in your system. Publish fairness reports, model evaluation results, and information about the ethical practices used to improve fairness.\nExplain your efforts to minimize biases and increase cultural sensitivity, demonstrating a commitment to ethical AI.\n7. Inclusive Design and Development\nInvolve diverse teams during the system design and development phases. This includes gathering input from individuals across different cultural, gender, and social backgrounds, which helps create a more inclusive system.\nRun user testing with a wide range of individuals from diverse communities to capture and address any concerns about biased or unfair recommendations.\nBy incorporating these approaches, you can enhance transparency and fairness in your AI system, fostering trust among users from diverse backgrounds.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<details><summary>Click here for hint</summary>\nConsider steps like conducting a bias assessment, enhancing diversity in training data, implementing explainability features, and establishing a user feedback loop to ensure fairness and transparency in your AI system. \n</details>\n\n<details><summary>Click here for sample solution</summary>\nTo address this issue, you could implement the following steps: \n\n1. Conduct a thorough bias assessment to identify and understand the biases present in the training data and algorithms. \n2. Use specialized tools or metrics to measure and quantify biases in content recommendations. \n3. Enhance the diversity of your training data by including a broader range of cultural, demographic, and user behavior data. \n4. Ensure that the training data reflects the diversity of your user base to reduce biases. \n5. Implement explainability features to provide users with insights into why specific recommendations are made. \n6. Offer transparency by showing the key factors and attributes influencing the recommendations. \n7. Establish a user feedback loop where users can report biased recommendations or provide feedback on content relevance. \n8. Regularly analyze this feedback to iteratively improve the system's fairness. \n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ">**Additional Information**\n\n>Some specialized tools that can be used to measure and quantify biases: \n\n>Holistic AI Library: This open-source library offers a range of metrics and mitigation strategies for various AI tasks, including content recommendation. It analyzes data for bias across different dimensions and provides visualizations for clear understanding. \n\n>Fairness 360: IBM&#39;s Fairness 360 toolkit provides various tools like Aequitas and What-If Tool to analyze bias in data sets, models, and decision-making processes. It offers metrics like statistical parity, differential odds ratio, and counterfactual fairness. IBM moved AI Fairness 360 to LF AI in July 2020. \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 2  \n\nYour company has deployed a chatbot powered by generative AI to interact with customers. The chatbot occasionally generates responses that are inappropriate or offensive, leading to customer dissatisfaction. As the AI developer, how do you take responsibility for these incidents and ensure accountability in the deployment of the AI chatbot? \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<i>Type your response here</i>\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "To take responsibility for the inappropriate or offensive responses generated by the AI chatbot and ensure accountability, several actions should be taken both to address the incidents and prevent them from happening in the future:\n1. Acknowledge and Address the Incidents\nImmediate Response to Customers: Apologize directly to the affected customers, acknowledging the issue and expressing your commitment to rectifying the problem.\nProvide Customer Support Options: Offer human intervention or support escalation when the chatbot fails to provide a satisfactory or appropriate response. Ensure customers can easily connect with a human agent for resolution.\n2. Implement Robust Content Moderation and Filtering\nBuild Real-Time Filters: Incorporate offensive language detection and filters to prevent inappropriate content from being generated or displayed. These filters should block harmful language, biased remarks, or sensitive topics.\nEstablish Safe Conversation Rules: Implement ethical guidelines and constraints on the chatbot’s language generation model, ensuring it adheres to company values and avoids inappropriate topics.\n3. Regularly Audit and Monitor Chatbot Behavior\nSet Up Monitoring Systems: Continuously monitor the chatbot’s conversations using automated tools to detect any undesirable responses. This allows you to catch inappropriate behavior early and take corrective action.\nConduct Bias Audits: Periodically audit the chatbot for any biases or offensive tendencies, paying particular attention to how it responds to sensitive topics like race, gender, and cultural differences. Use this data to refine the system.\n4. Improve and Retrain the AI Model\nRetrain the Model with Better Data: Collect feedback from incidents of inappropriate responses and use this to improve the training data. Exclude content that may lead to harmful or offensive behavior and introduce more diverse, respectful data.\nIncorporate Human-in-the-Loop (HITL) Learning: Implement HITL systems where human reviewers can intervene to correct problematic responses and retrain the AI based on real-world conversations.\n5. Establish Clear Accountability and Ethics Guidelines\nDefine Roles and Responsibilities: As the AI developer, take responsibility for the system’s performance and create a clear escalation protocol to address harmful outputs. Ensure the team knows who is accountable for maintaining and updating the chatbot.\nCreate Ethical Guidelines: Develop and enforce clear ethical standards for the chatbot’s behavior, ensuring it aligns with company values and customer expectations.\n6. Enable Customer Feedback Mechanisms\nProvide Feedback Options: Allow users to rate chatbot interactions and provide feedback when they encounter an inappropriate or unsatisfactory response. Use this feedback to fine-tune the system.\nAutomated Alerts for Harmful Responses: Build mechanisms to detect when customers flag inappropriate responses, enabling faster reaction and resolution of such issues.\n7. Limit the Chatbot’s Scope and Knowledge Base\nAvoid Sensitive Topics: Restrict the chatbot from engaging in certain sensitive topics like politics, religion, or controversial issues to avoid offending users. Set explicit boundaries on what the chatbot can and cannot discuss.\nPredefined Templates for Sensitive Scenarios: For potentially sensitive queries, use predefined, well-moderated response templates rather than relying solely on generative AI.\n8. Transparency with Customers\nDisclose Limitations: Clearly inform users about the chatbot’s limitations, stating that it may not always provide perfect responses and that human assistance is available when needed.\nExplain Actions Taken: Be transparent with customers about the steps taken to prevent future inappropriate behavior, demonstrating that you are actively working to improve the system.\n9. Collaborate with Legal and Compliance Teams\nEnsure Regulatory Compliance: Work with legal and compliance teams to ensure the chatbot adheres to data privacy laws, industry standards, and regulations related to content moderation and AI ethics.\nBy taking these steps, you demonstrate accountability and a proactive approach to improving the chatbot’s reliability and ethical behavior, ensuring that customer satisfaction and trust are restored and maintained.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<details><summary>Click here for hint</summary>\nTo address responsibility and accountability, analyze errors, respond swiftly, continuously monitor for inappropriate responses, and communicate openly with stakeholders about corrective actions taken to improve the chatbot&#39;s behavior. \n</details>\n\n<details><summary>Click here for sample solution</summary>\nAddressing responsibility and accountability in this scenario involves the following steps: \n\n1. Conduct a detailed analysis of the inappropriate responses to identify patterns and root causes. \n2. Determine whether the issues stem from biased training data, algorithmic limitations, or other factors. \n3. Implement a mechanism to quickly identify and rectify inappropriate responses by updating the chatbot&#39;s training data or fine-tuning the model. \n4. Communicate openly with affected customers, acknowledge the issue, and assure them of prompt corrective actions. \n5. Set up continuous monitoring systems to detect and flag inappropriate responses in real-time. \n6. Implement alerts or human-in-the-loop mechanisms to intervene when the system generates potentially harmful content. \n7. Clearly communicate the steps taken to address the issue to both internal stakeholders and customers. \n8. Emphasize the commitment to continuous improvement and the responsible use of AI technology. \n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Exercise 3: \n\nYour company has developed a generative AI model that autonomously generates product descriptions for an e-commerce platform. However, users have reported instances where the generated descriptions contain inaccurate information, leading to customer confusion and dissatisfaction. How do you enhance the reliability of your AI model to ensure accurate product descriptions? \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<i>Type your response here</i>\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "Enhancing the reliability of the generative AI model to ensure accurate product descriptions requires a multi-faceted approach that involves improving data quality, refining the model’s logic, and incorporating human oversight. Here’s how to address this issue effectively:\n1. Improve Data Quality and Input\nEnsure Accurate and Up-to-Date Data Sources: Ensure that the AI model has access to accurate, comprehensive, and up-to-date product information. Integrate the system with reliable product databases and APIs to provide correct specifications, features, and attributes for each item.\nValidate Product Information: Implement checks to verify that the product data provided to the model is accurate and complete before it generates descriptions. Missing or incorrect data should trigger alerts for further review or manual input.\n2. Enhance the AI Model’s Training Process\nRefine the Training Data: Retrain the model using high-quality, accurate examples of product descriptions. Eliminate or limit the influence of low-quality or inaccurate data to reduce the likelihood of errors in the generated content.\nDomain-Specific Fine-Tuning: Fine-tune the model on domain-specific knowledge, ensuring that it understands the nuances of the products and categories it describes. For example, a clothing description model should understand fabric types, sizes, and styles, while a tech product model should grasp specifications like processor speed and storage capacity.\n3. Introduce Structured Templates for Key Details\nUse Structured Information for Critical Fields: For critical product information such as dimensions, materials, technical specifications, or warranty details, use structured fields rather than relying solely on generative AI. This ensures key details are always accurate.\nIncorporate Hybrid Approach: Combine generative text with template-based or rule-based sections for factual product data. This allows the model to generate creative parts of the description while ensuring accuracy for essential product details.\n4. Implement Real-Time Data Verification\nCross-Check with Product Databases: Implement an automated system that cross-checks generated descriptions with the original product database to identify and correct discrepancies in real-time.Cross-Check with Product Databases: Implement an automated system that cross-checks generated descriptions with the original product database to identify and correct discrepancies in real-time.\nFlagging System for Uncertain Outputs: Incorporate confidence scores into the AI’s output generation. If the system is uncertain about certain details or deviates from established patterns, it should flag the output for human review before publication.\n5. Introduce Human-in-the-Loop (HITL) Validation\nEnable Human Oversight: Implement a human-in-the-loop approach where human reviewers validate or correct AI-generated descriptions before they are published, especially for complex or high-value products.\nSpot-Check Random Samples: Regularly conduct spot checks of a random sample of descriptions to detect patterns of inaccuracies and address them before they become widespread.\n6. Continuous Model Monitoring and Feedback Loop\nMonitor Post-Deployment Performance: Continuously monitor the model’s performance by tracking user feedback, error rates, and patterns in inaccurate descriptions. Use this data to fine-tune the model further.\nUser Feedback Integration: Allow users (e.g., product managers or customers) to flag descriptions that seem inaccurate, and feed this information back into the system to improve future outputs.User Feedback Integration: Allow users (e.g., product managers or customers) to flag descriptions that seem inaccurate, and feed this information back into the system to improve future outputs.\n7. Contextual Awareness and Constraints\nSet Logical Constraints: Build product-specific constraints or rules into the model. For example, prevent the model from describing a laptop as having \"infinite battery life\" or stating that a cotton shirt is made from polyester.\nImprove Contextual Understanding: Enhance the model’s understanding of product categories, making sure it understands context and relationships between different product attributes (e.g., a 40-inch TV should not be described as \"pocket-sized\").\n8. Testing and Validation Before Deployment\nRigorous Testing with Diverse Products: Before deploying the model to generate live descriptions, run tests across a wide range of products, including edge cases, to ensure the model performs reliably in all scenarios.\nA/B Testing with Descriptions: Use A/B testing to compare AI-generated descriptions with human-written descriptions. Monitor customer engagement, feedback, and accuracy, and iteratively improve the AI’s output based on these insights.\n9. Establish Clear Accountability\nCreate Accountability Mechanisms: Assign responsibility to a team for reviewing the AI system’s output, addressing customer-reported inaccuracies, and regularly updating the model and data sources.\nTransparency with Users: Communicate with customers that product descriptions are AI-generated and encourage them to report any issues. This fosters transparency and allows for quicker correction of errors.\nBy following these strategies, you can significantly improve the reliability and accuracy of the AI model, ensuring that product descriptions align with factual data and enhance customer trust in the platform.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<details><summary>Click here for hint</summary>\nTo improve reliability, focus on quality assurance testing, use domain-specific training data, adopt an iterative model training approach, and integrate user feedback to iteratively correct errors and enhance the accuracy of the AI-generated product descriptions. \n</details>\n\n<details><summary>Click here for sample solution</summary>\nTo improve the reliability of the AI model in generating product descriptions, consider the following actions: \n\n1. Implement rigorous quality assurance testing to evaluate the accuracy of the generated product descriptions. \n2. Create a comprehensive testing data set that covers a wide range of products and scenarios to identify and address inaccuracies. \n3. Ensure that the AI model is trained on a diverse and extensive data set specific to the e-commerce domain. \n4. Include product information from reputable sources to enhance the model's understanding of accurate product details. \n5. Implement an iterative training approach to continuously update and improve the model based on user feedback and evolving product data. \n6. Regularly retrain the model to adapt to changes in the product catalog and user preferences. \n7. Encourage users to provide feedback on inaccurate product descriptions. \n8. Use this feedback to fine-tune the model, correct errors, and improve the overall reliability of the AI-generated content.  \n</details>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "---\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Congratulations! You have completed the lab\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Authors\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "[Dr. Pooja](https://www.linkedin.com/in/p-b28802262/)\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Other Contributors\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "[Abhishek Gagneja](https://www.linkedin.com/in/abhishek-gagneja-23051987/)\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<!--## Change Log--!>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<!--|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n|-|-|-|-|\n|2023-12-14|0.1|Abhishek Gagneja|Initial Draft created| --!>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Copyright © IBM Corporation. All rights reserved.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    }
  ]
}